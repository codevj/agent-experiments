{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use ChatOllama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO - Automate installation of Ollama for local devbox setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llama3.2:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Je aime le programmation.', additional_kwargs={}, response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-01-26T05:31:19.643641Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1720113417, 'load_duration': 559992459, 'prompt_eval_count': 45, 'prompt_eval_duration': 833000000, 'eval_count': 7, 'eval_duration': 54000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-88153d63-1894-48f6-9f07-6db8eb9e5858-0', usage_metadata={'input_tokens': 45, 'output_tokens': 7, 'total_tokens': 52})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=model_name,\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ich liebe Programmierung.', additional_kwargs={}, response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-01-26T05:31:23.086124Z', 'done': True, 'done_reason': 'stop', 'total_duration': 217513750, 'load_duration': 32372375, 'prompt_eval_count': 40, 'prompt_eval_duration': 139000000, 'eval_count': 6, 'eval_duration': 44000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-db95ef2e-d415-4850-9ce7-420a28c1e724-0', usage_metadata={'input_tokens': 40, 'output_tokens': 6, 'total_tokens': 46})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'validate_user',\n",
       "  'args': {'addresses': ['123 Fake St', '234 Pretend Boulevard'],\n",
       "   'user_id': 123},\n",
       "  'id': '5384b9fe-29b8-4697-ace6-7d20c67f9546',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "@tool\n",
    "def validate_user(user_id: int, addresses: List[str]) -> bool:\n",
    "    \"\"\"Validate user using historical addresses.\n",
    "\n",
    "    Args:\n",
    "        user_id (int): the user ID.\n",
    "        addresses (List[str]): Previous addresses as a list of strings.\n",
    "    \"\"\"\n",
    "    return True\n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=model_name,\n",
    "    temperature=0,\n",
    ").bind_tools([validate_user])\n",
    "\n",
    "result = llm.invoke(\n",
    "    \"Could you validate user 123? They previously lived at \"\n",
    "    \"123 Fake St in Boston MA and 234 Pretend Boulevard in \"\n",
    "    \"Houston TX.\"\n",
    ")\n",
    "result.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug tool calling by introduing DebuggableChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Debug: Binding Tools ---\n",
      "Binding Tool: name='validate_user' description='Validate user using historical addresses.\\n\\n    Args:\\n        user_id (int): the user ID.\\n        addresses (List[str]): Previous addresses as a list of strings.' args_schema=<class 'langchain_core.utils.pydantic.validate_user'> func=<function validate_user at 0x137cbfb00>\n",
      "--- End Debug ---\n",
      "\n",
      "\n",
      "--- Debug: Sending Prompt ---\n",
      "Could you validate user 123? They previously lived at \n",
      "--- End Debug ---\n",
      "\n",
      "\n",
      "--- Debug: LLM Response ---\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.3:latest', 'created_at': '2025-01-26T05:31:40.642743Z', 'done': True, 'done_reason': 'stop', 'total_duration': 9562956542, 'load_duration': 810959083, 'prompt_eval_count': 209, 'prompt_eval_duration': 4337000000, 'eval_count': 36, 'eval_duration': 4412000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-6baaedc5-fb54-4717-b756-02deb4e37585-0' tool_calls=[{'name': 'validate_user', 'args': {'addresses': ['1313 Mockingbird Lane', '42 Wallaby Way'], 'user_id': 123}, 'id': '1e50c395-7f19-4e39-a0d4-06699905c277', 'type': 'tool_call'}] usage_metadata={'input_tokens': 209, 'output_tokens': 36, 'total_tokens': 245}\n",
      "--- End Debug ---\n",
      "\n",
      "Final Response: content='' additional_kwargs={} response_metadata={'model': 'llama3.3:latest', 'created_at': '2025-01-26T05:31:40.642743Z', 'done': True, 'done_reason': 'stop', 'total_duration': 9562956542, 'load_duration': 810959083, 'prompt_eval_count': 209, 'prompt_eval_duration': 4337000000, 'eval_count': 36, 'eval_duration': 4412000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-6baaedc5-fb54-4717-b756-02deb4e37585-0' tool_calls=[{'name': 'validate_user', 'args': {'addresses': ['1313 Mockingbird Lane', '42 Wallaby Way'], 'user_id': 123}, 'id': '1e50c395-7f19-4e39-a0d4-06699905c277', 'type': 'tool_call'}] usage_metadata={'input_tokens': 209, 'output_tokens': 36, 'total_tokens': 245}\n"
     ]
    }
   ],
   "source": [
    "class DebuggableChatOllama:\n",
    "    def __init__(self, llm):\n",
    "        \"\"\"\n",
    "        Initialize the DebuggableChatOllama proxy for debugging interactions with the LLM.\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.bound_tools = None\n",
    "\n",
    "    def bind_tools(self, tools):\n",
    "        \"\"\"\n",
    "        Bind tools to the LLM and log the tools being bound.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Debug: Binding Tools ---\")\n",
    "        for tool in tools:\n",
    "            print(f\"Binding Tool: {tool.__name__ if hasattr(tool, '__name__') else str(tool)}\")\n",
    "        print(\"--- End Debug ---\\n\")\n",
    "        self.bound_tools = self.llm.bind_tools(tools)\n",
    "        return self\n",
    "\n",
    "    def __call__(self, prompt):\n",
    "        \"\"\"\n",
    "        Log and invoke the prompt on the bound LLM.\n",
    "        \"\"\"\n",
    "        if self.bound_tools is None:\n",
    "            raise RuntimeError(\"Tools have not been bound. Call bind_tools first.\")\n",
    "        \n",
    "        print(\"\\n--- Debug: Sending Prompt ---\")\n",
    "        print(prompt)\n",
    "        print(\"--- End Debug ---\\n\")\n",
    "        \n",
    "        response = self.bound_tools.invoke(prompt)\n",
    "        \n",
    "        print(\"\\n--- Debug: LLM Response ---\")\n",
    "        print(response)\n",
    "        print(\"--- End Debug ---\\n\")\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"llama3.3:latest\"  # Replace with your model name\n",
    "    base_llm = ChatOllama(\n",
    "        model=model_name,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    debug_llm = DebuggableChatOllama(base_llm)\n",
    "    debug_llm = debug_llm.bind_tools([validate_user])\n",
    "\n",
    "    prompt = \"Could you validate user 123? They previously lived at \"\n",
    "    \"123 Fake St in Boston MA and 234 Pretend Boulevard in \"\n",
    "    \"Houston TX.\"\n",
    "    response = debug_llm(prompt)\n",
    "    print(f\"Final Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More tool calling\n",
    "\n",
    "### Python functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.3:latest', 'created_at': '2025-01-26T05:31:51.545194Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5304960417, 'load_duration': 30338500, 'prompt_eval_count': 228, 'prompt_eval_duration': 2641000000, 'eval_count': 22, 'eval_duration': 2632000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-587a499e-6a2a-4872-a549-3bf9adae3714-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': '148ea80e-207c-45d1-8563-4e1d1db46df4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 228, 'output_tokens': 22, 'total_tokens': 250})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The function name, type hints, and docstring are all part of the tool\n",
    "# schema that's passed to the model. Defining good, descriptive schemas\n",
    "# is an extension of prompt engineering and is an important part of\n",
    "# getting models to perform well.\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools = [add, multiply]\n",
    "llm = ChatOllama(\n",
    "    model=model_name,\n",
    "    temperature=0,\n",
    ")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "query = \"What is 3 * 12?\"\n",
    "\n",
    "llm_with_tools.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pydantic class\n",
    "\n",
    "You can bind tools using Pydantic classes as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.3:latest', 'created_at': '2025-01-26T05:31:58.080368Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3739856417, 'load_duration': 23681917, 'prompt_eval_count': 228, 'prompt_eval_duration': 1105000000, 'eval_count': 22, 'eval_duration': 2610000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-96808d02-aed6-45e1-a937-b7818e2c9799-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': '2b52ad0d-3543-4787-9b71-eea4497eb4a5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 228, 'output_tokens': 22, 'total_tokens': 250})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class add(BaseModel):\n",
    "    \"\"\"Add two integers.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "class multiply(BaseModel):\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "llm = ChatOllama(\n",
    "    model=model_name,\n",
    "    temperature=0,\n",
    ")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "query = \"What is 3 * 12?\"\n",
    "\n",
    "llm_with_tools.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TypedDict class\n",
    "\n",
    "You can bind tools using TypedDicts and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.3:latest', 'created_at': '2025-01-26T05:32:03.843935Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3856594708, 'load_duration': 28243958, 'prompt_eval_count': 228, 'prompt_eval_duration': 1202000000, 'eval_count': 22, 'eval_duration': 2625000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-2821ab2e-8b96-4e1c-8dc3-4154348d8dad-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': '5cd8cf6f-400a-480c-9e5a-837bffe73d10', 'type': 'tool_call'}], usage_metadata={'input_tokens': 228, 'output_tokens': 22, 'total_tokens': 250})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class add(TypedDict):\n",
    "    \"\"\"Add two integers.\"\"\"\n",
    "\n",
    "    # Annotations must have the type and can optionally include a default value and description (in that order).\n",
    "    a: Annotated[int, ..., \"First integer\"]\n",
    "    b: Annotated[int, ..., \"Second integer\"]\n",
    "\n",
    "\n",
    "class multiply(TypedDict):\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "\n",
    "    a: Annotated[int, ..., \"First integer\"]\n",
    "    b: Annotated[int, ..., \"Second integer\"]\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "llm = ChatOllama(\n",
    "    model=model_name,\n",
    "    temperature=0,\n",
    ")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "query = \"What is 3 * 12?\"\n",
    "\n",
    "llm_with_tools.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 3, 'b': 12},\n",
       "  'id': 'da29abea-b957-45b0-a03e-171a83c3b0fe',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'add',\n",
       "  'args': {'a': 11, 'b': 49},\n",
       "  'id': '61039664-545e-4fee-a2bd-ef20e71af5e1',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "\n",
    "result = llm_with_tools.invoke(query)\n",
    "\n",
    "result.tool_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'multiply', 'args': {'__arg1': 3, '__arg2': 12}, 'id': '1592a762-9cb7-4ce0-8283-e7c9a11c4cb4', 'type': 'tool_call'}, {'name': 'add', 'args': {'__arg1': 11, '__arg2': 49}, 'id': 'f858dff2-749c-4313-9450-6631bf79c69f', 'type': 'tool_call'}]\n",
      "content='What is 3 * 12? Also, what is 11 + 49?' additional_kwargs={} response_metadata={} role='human'\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-01-26T05:32:14.183382Z', 'done': True, 'done_reason': 'stop', 'total_duration': 690102125, 'load_duration': 24280334, 'prompt_eval_count': 214, 'prompt_eval_duration': 211000000, 'eval_count': 50, 'eval_duration': 453000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-29d03a80-4828-404e-92cc-4ba5b6ddb0d9-0' tool_calls=[{'name': 'multiply', 'args': {'__arg1': 3, '__arg2': 12}, 'id': '1592a762-9cb7-4ce0-8283-e7c9a11c4cb4', 'type': 'tool_call'}, {'name': 'add', 'args': {'__arg1': 11, '__arg2': 49}, 'id': 'f858dff2-749c-4313-9450-6631bf79c69f', 'type': 'tool_call'}] usage_metadata={'input_tokens': 214, 'output_tokens': 50, 'total_tokens': 264}\n",
      "content='36' tool_call_id='1592a762-9cb7-4ce0-8283-e7c9a11c4cb4' tool='multiply'\n",
      "content='60' tool_call_id='f858dff2-749c-4313-9450-6631bf79c69f' tool='add'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import Tool\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "\n",
    "# Define tool functions\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# Define tools\n",
    "tools = [\n",
    "    Tool(name=\"add\", description=\"Add two numbers.\", func=add),\n",
    "    Tool(name=\"multiply\", description=\"Multiply two numbers.\", func=multiply),\n",
    "]\n",
    "\n",
    "# Initialize the LLM\n",
    "model_name = \"llama3.2:latest\"  # Replace with your actual model name\n",
    "llm = ChatOllama(\n",
    "    model=model_name,\n",
    "    temperature=0,\n",
    ")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Define the query and wrap it in a HumanMessage\n",
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "messages = [HumanMessage(content=query, role=\"human\")]\n",
    "\n",
    "# Invoke the LLM with tools\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "\n",
    "# Output the tool calls made by the LLM\n",
    "print(ai_msg.tool_calls)\n",
    "\n",
    "# Append the AI's response to the messages\n",
    "messages.append(ai_msg)\n",
    "\n",
    "# Process tool calls\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    # Extract tool name and arguments\n",
    "    tool_name = tool_call[\"name\"].lower()\n",
    "    tool_args = tool_call[\"args\"]\n",
    "    tool_call_id = tool_call[\"id\"]  # Extract the tool_call_id\n",
    "\n",
    "    # Map `__arg1` and `__arg2` to `a` and `b`\n",
    "    mapped_args = {\n",
    "        \"a\": tool_args.get(\"__arg1\"),\n",
    "        \"b\": tool_args.get(\"__arg2\"),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Call the tool with the mapped arguments\n",
    "        selected_tool = {\"add\": add, \"multiply\": multiply}[tool_name]\n",
    "        result = selected_tool(**mapped_args)\n",
    "    except Exception as e:\n",
    "        result = f\"Error processing tool call '{tool_name}': {e}\"\n",
    "\n",
    "    # Create a tool message to append to messages\n",
    "    tool_msg = ToolMessage(\n",
    "        tool=tool_name,\n",
    "        content=str(result),  # Convert result to string for compatibility\n",
    "        tool_call_id=tool_call_id,  # Pass the tool_call_id\n",
    "    )\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "# Inspect the updated conversation\n",
    "for message in messages:\n",
    "    print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The result of 3 * 12 is 36.\\n\\nThe result of 11 + 49 is 60.', additional_kwargs={}, response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-01-26T05:32:17.923489Z', 'done': True, 'done_reason': 'stop', 'total_duration': 513032000, 'load_duration': 29036250, 'prompt_eval_count': 133, 'prompt_eval_duration': 267000000, 'eval_count': 25, 'eval_duration': 215000000, 'message': Message(role='assistant', content='The result of 3 * 12 is 36.\\n\\nThe result of 11 + 49 is 60.', images=None, tool_calls=None)}, id='run-955730b9-23ca-49ef-b7d1-53895118089b-0', usage_metadata={'input_tokens': 133, 'output_tokens': 25, 'total_tokens': 158})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(messages)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-experiments-KuwhBv9--py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
